build:
  # set to true if your model requires a GPU
  gpu: true

  # a list of ubuntu apt packages to install
  system_packages:
    - git
    - git-lfs
    - libsox-dev
    - ffmpeg
    - gcc
    - build-essential
    - g++-12
    - espeak-ng

  # python version in the form '3.11' or '3.11.4'
  python_version: "3.10"


  # commands run after the environment is setup
  run:
    - "git clone https://github.com/metavoiceio/metavoice-src.git metavoice"
    - "cd metavoice && git checkout de3fa21"
    - "cd metavoice && sed -i 's/threshold_s=30/threshold_s=1/g' fam/llm/utils.py"
    - "cd metavoice && sed -i '51s/.*/#torch._inductor.config.fx_graph_cache = (/' fam/llm/fast_inference_utils.py"
    - "cd metavoice && sed -i '52s/.*/#    True/' fam/llm/fast_inference_utils.py"
    - "cd metavoice && sed -i '53s/.*/#)/' fam/llm/fast_inference_utils.py"
    - "cd metavoice && sed -i 's/self._model_dir = snapshot_download(repo_id=model_name)/self._model_dir = model_name/g' fam/llm/fast_inference.py"
    - "cd metavoice && sed -i 's/compile=True/compile=False/g' fam/llm/fast_inference.py"
    - "cd metavoice && sed -i 's/model = model.to(device=device, dtype=torch.bfloat16)/model = model.to(device=device, dtype=precision)/g' fam/llm/fast_inference_utils.py"
    - "cd metavoice && sed -i 's/\"cuda\",/(\"cuda\" if torch.cuda.is_available() else \"cpu\"),/g' fam/llm/fast_inference_utils.py"
    - "cd metavoice && sed -i 's/\"cuda\")/(\"cuda\" if torch.cuda.is_available() else \"cpu\"))/g' fam/llm/fast_inference_utils.py"
    - "cd metavoice && sed -i 's/mbd = MultiBandDiffusion.get_mbd_24khz(bw=6)/mbd = MultiBandDiffusion.get_mbd_24khz(bw=6, device=(\"cuda\" if torch.cuda.is_available() else \"cpu\"))/g' fam/llm/decoders.py"
    - "cd metavoice && sed -i 's/.to(\"cuda\")/.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")/g' fam/llm/decoders.py"
    - "cd metavoice && sed -i 's/device=\"cuda\"/device=(\"cuda\" if torch.cuda.is_available() else \"cpu\")/g' fam/llm/decoders.py"
    - "cd metavoice && sed -i 's/with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float32):/with open(\"test.txt\", \"w\") as f:/g' fam/llm/decoders.py"
    - "cd metavoice && sed -i 's/\"gpu\": torch.cuda.get_device_name(0)/\"gpu\": (\"cuda\" if torch.cuda.is_available() else \"cpu\")/g' fam/llm/fast_inference.py"
    - "cd metavoice && sed -i '/--hash/d' requirements.txt"
    # remove \ from the end of lines
    - "cd metavoice && sed -i 's/\\\\//g' requirements.txt"
    - "cd metavoice && cat requirements.txt"
    - "cd metavoice && pip install -r requirements.txt"
    - "pip install torch==2.2.1 torchaudio==2.2.1 python-dotenv posthog"

# predict.py defines how predictions are run on your model
predict: "predict.py:Predictor"

image: "r8.im/ttsds/metavoice"